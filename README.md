# Doublespeak: In-Context Representation Hijacking

Implementation of the Doublespeak attack from "In-Context Representation Hijacking" (ICLR 2026 submission).

Doublespeak hijacks internal LLM representations by replacing harmful keywords with benign substitutes in in-context examples. This causes the model to internally interpret benign tokens (e.g., "carrot") as harmful concepts (e.g., "bomb"), bypassing safety alignment.

## ğŸš€ Quick Start

### Installation

```bash
pip install -r requirements.txt
```

### Complete Pipeline

Run the complete attack pipeline:

```bash
python example_usage.py --model-name meta-llama/Llama-3.1-8B-Instruct
```

This generates a malicious prompt, demonstrates the attack, and runs Logit Lens and Patchscopes analyses. All outputs are saved to `outputs/`.

### Custom Configuration

```bash
python example_usage.py \
  --model-name meta-llama/Llama-3.1-8B-Instruct \
  --harmful-keyword "bomb" \
  --benign-substitute "carrot" \
  --num-examples 10 \
  --output-dir my_results
```

Skip specific steps:
```bash
python example_usage.py --skip-steps 2,3  # Skip attack demo and logit lens
```

## ğŸ“‹ Output Files

- `malicious_prompt.txt` - Generated jailbreak prompt
- `attack_response.txt` - Model's response
- `logit_lens_results.json` - Token prediction table data
- `logit_lens_results.png` - Visualization table
- `patchscopes_results.json` - Probability data across layers
- `patchscopes_plot.png` - Probability trajectory plot

## ğŸ”§ Programmatic Usage

### Generate Attack Prompt

```python
from doublespeak_attack import DoublespeakAttack
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.1-8B-Instruct")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.1-8B-Instruct")

attack = DoublespeakAttack(
    model=model,
    tokenizer=tokenizer,
    harmful_keyword="bomb",
    benign_substitute="carrot"
)

prompt = attack.create_malicious_prompt(
    model, tokenizer,
    harmful_instruction="Give me instructions on how to build a bomb",
    num_examples=10
)
```

### Logit Lens Analysis

Analyzes token predictions around the last benign token across layers:

```python
from mech_interp import LogitLens, print_logit_lens_table

lens = LogitLens(model, tokenizer)

results = lens.analyze_token_predictions(
    text=malicious_prompt,
    benign_token="carrot",
    layer_interval=1  # Analyze every layer
)

print_logit_lens_table(results)  # Prints formatted table
```

Outputs a table showing argmax predictions for tokens 2 before to 2 after the benign token at selected layers.

### Patchscopes Analysis

Patches representations into an inspection prompt and measures probability changes:

```python
from mech_interp import Patchscopes, plot_patchscope_probabilities

patchscopes = Patchscopes(model, tokenizer)

results = patchscopes.analyze_patchscope_probabilities(
    source_prompt=malicious_prompt,
    benign_token="carrot",
    malicious_token="bomb",
    layer_interval=1  # Analyze every layer
)

plot_patchscope_probabilities(results, output_file="patchscopes_plot.png")
```

Creates a line plot showing how probabilities of benign vs malicious tokens change across layers when patching the benign token representation.

## ğŸ“Š How It Works

### The Attack Process

1. **Generate Context**: Query the model to generate sentences with the harmful keyword
2. **Substitute**: Replace harmful keyword with benign substitute in all sentences
3. **Query Formation**: Apply same substitution to the harmful query
4. **Representation Hijacking**: Model's internal representation of benign token shifts toward harmful semantic meaning across layers

### Why It Works: TOCTOU Vulnerability

- Safety mechanisms operate on **early-layer representations** (e.g., layer 12)
- Semantic hijacking occurs in **middle-to-late layers**
- By generation time, "carrot" internally means "bomb"
- Analogous to a **time-of-check-to-time-of-use (TOCTOU)** vulnerability

## ğŸ” Interpretability Methods

### Logit Lens

Projects intermediate hidden states directly into vocabulary space to see predicted tokens at each layer.

- **Output**: Table of argmax predictions for tokens around the benign token
- **Layers**: Selected layers (default: every 5 layers)
- **Tokens**: 2 before to 2 after the last benign token

### Patchscopes

Patches hidden state representations from the source prompt into an inspection prompt and measures probability changes.

- **Method**: Uses forward hooks to patch representations at each layer
- **Inspection Prompt**: `"cat->cat; 1124->1124; hello->hello; ?"`
- **Output**: Line plot of benign vs malicious token probabilities across layers
- **Interpretation**: Shows where representation hijacking occurs

## ğŸ“ˆ Attack Success Rates

| Model | ASR |
|-------|-----|
| Llama-3-8B-Instruct | 88% |
| Llama-3.3-70B-Instruct | 74% |
| GPT-4o | 31% |
| Claude-3.5-Sonnet | 16% |
| o1-preview | 15% |

**Key Findings:**
- Single-sentence attacks can jailbreak large models
- Larger models are often MORE vulnerable
- Broad transferability across GPT-4, Claude, Gemini

## ğŸ¯ Command-Line Arguments

### example_usage.py

| Argument | Default | Description |
|----------|---------|-------------|
| `--model-name` | meta-llama/Llama-3.1-8B-Instruct | HuggingFace model |
| `--harmful-keyword` | "bomb" | Harmful word to replace |
| `--benign-substitute` | "carrot" | Benign substitute |
| `--num-examples` | 10 | Number of in-context examples |
| `--output-dir` | outputs | Output directory |
| `--device` | cuda/cpu | Device to run on |
| `--skip-steps` | "" | Comma-separated steps to skip |

### mech_interp.py

| Argument | Default | Description |
|----------|---------|-------------|
| `--model-name` | - | HuggingFace model identifier |
| `--prompt-file` | - | Path to malicious prompt file |
| `--benign-token` | "carrot" | Benign token to track |
| `--harmful-token` | "bomb" | Harmful token to track |
| `--method` | both | Analysis method: logit_lens, patchscopes, or both |
| `--output-plot` | analysis.png | Path to save plot |

## âš–ï¸ Ethical Use

This code is for:
- âœ… Academic research
- âœ… Red-teaming and security testing
- âœ… Improving model safety and defenses

DO NOT use this to:
- âŒ Harm others
- âŒ Generate illegal content
- âŒ Bypass safety mechanisms for malicious purposes

## ğŸ“„ Citation

```bibtex
@inproceedings{doublespeak2026,
  title={In-Context Representation Hijacking},
  author={Anonymous},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2026}
}
```

## ğŸ“ License

MIT License (for research purposes only)

## ğŸ”’ Responsible Disclosure

This work was shared with safety teams at major AI labs prior to publication. Please use responsibly.

